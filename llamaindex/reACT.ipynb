{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for a ReAct Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Set up observability with Llamatrace¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import (\n",
    "    OTLPSpanExporter as HTTPSpanExporter,\n",
    ")\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "PHOENIX_API_KEY = os.getenv(\"OTEL_EXPORTER_OTLP_HEADERS\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Add Phoenix\n",
    "span_phoenix_processor = SimpleSpanProcessor(\n",
    "    HTTPSpanExporter(endpoint=\"https://app.phoenix.arize.com/v1/traces\")\n",
    ")\n",
    "\n",
    "# Add them to the tracer\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(span_processor=span_phoenix_processor)\n",
    "\n",
    "# Instrument the application\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the workflow\n",
    "An agent consists of several steps\n",
    "1. Handling the latest incoming user message, including adding to memory and preparing the chat history\n",
    "2. Using the chat history and tools to construct a ReAct prompt\n",
    "3. Calling the llm with the react prompt, and parsing out function/tool calls\n",
    "4. If no tool calls, we can return\n",
    "5. If there are tool calls, we need to execute them, and then loop back for a fresh ReAct prompt using the latest tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Workflow Events\n",
    "To handle these steps, we need to define a few events:\n",
    "\n",
    "1. An event to handle new messages and prepare the chat history\n",
    "2. An event to prompt the LLM with the react prompt\n",
    "3. An event to trigger tool calls, if any\n",
    "4. An event to handle the results of tool calls, if any\n",
    "\n",
    "The other steps will use the built-in StartEvent and StopEvent events.\n",
    "\n",
    "In addition to events, we will also use the global context to store the current react reasoning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class PrepEvent(Event): #PrepEvent marks readiness for the next step.\n",
    "    pass\n",
    "\n",
    "class InputEvent(Event): #Represents the chat history (a list of messages), which the agent uses to prepare the ReAct prompt. | InputEvent carries the formatted prompt to the LLM.\n",
    "    input : list[ChatMessage]\n",
    "\n",
    "class ToolCallEvent(Event): # Signals that tools (or functions) need to be invoked as part of the reasoning process. | ToolCallEvent handles tool-related reasoning (e.g., calculations, queries).\n",
    "    tool_calls : list[ToolSelection] \n",
    "    \n",
    "class FunctionOutputEvent(Event): #Represents the results of invoking tools.\n",
    "    output: ToolOutput\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Workflow Itself\n",
    "With our events defined, we can construct our workflow and steps.\n",
    "\n",
    "Note that the workflow automatically validates itself using type annotations, so the type annotations on our steps are very helpful!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from typing import Any\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser\n",
    "from llama_index.core.agent.react.types import (\n",
    "    ActionReasoningStep, \n",
    "    ObservationReasoningStep\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking Down the @step Decorator and Workflow Concepts\n",
    "The @step decorator in the ReActAgent class marks functions as workflow steps. These steps represent individual units of work in the workflow. Let’s address your questions systematically.\n",
    "\n",
    "**How Do the Steps Work?**\n",
    "\n",
    "Each step:\n",
    "\n",
    "1. Takes input in the form of an event (e.g., StartEvent, PrepEvent, InputEvent).\n",
    "2. Processes the input data and performs some operations (e.g., storing user input, formatting chat history, or handling tool calls).\n",
    "3. Produces and returns a new event (e.g., PrepEvent, InputEvent) that acts as the output for the next step in the workflow.\n",
    "\n",
    "Think of it as a pipeline:\n",
    "\n",
    "Each step takes an event as input, processes it, and returns another event for the next step.\n",
    "\n",
    "\n",
    "**What Are ev and ctx?**\n",
    "\n",
    "- ev (Event):\n",
    "\n",
    "    - Represents the input event for the current step.\n",
    "    - Contains the data that the current step needs to operate on.\n",
    "    - Example: In new_user_msg, ev is a StartEvent that provides the input (the user's message).\n",
    "- ctx (Context):\n",
    "\n",
    "    - Acts as shared memory for the workflow.\n",
    "    - Allows steps to share intermediate data that isn’t directly passed through events.\n",
    "    - You can:\n",
    "        - Store data: await ctx.set(\"key\", value)\n",
    "        - Retrieve data: await ctx.get(\"key\")\n",
    "      \n",
    "These two mechanisms (ev and ctx) ensure steps can communicate with each other.\n",
    "\n",
    "\n",
    "**How Do the Steps Communicate?**\n",
    "\n",
    "- Direct Communication:\n",
    "\n",
    "    - One step returns an event (e.g., PrepEvent), and the next step receives it as input.\n",
    "    - Example:\n",
    "        - new_user_msg returns a PrepEvent.\n",
    "        - prepare_chat_history takes PrepEvent as input and uses it to proceed.\n",
    "        \n",
    "- Shared State (Context):\n",
    "\n",
    "    - Steps use ctx to share data that persists across multiple steps.\n",
    "    - Example:\n",
    "        - new_user_msg clears current_reasoning in ctx.\n",
    "        - Later steps like prepare_chat_history or handle_llm_input retrieve or update current_reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReActAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: LLM | None = None,\n",
    "        tools: list[BaseTool] | None = None,\n",
    "        extra_context: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.formatter = ReActChatFormatter(context=extra_context or \"\")\n",
    "        self.output_parser = ReActOutputParser()\n",
    "        self.sources = []\n",
    "\n",
    "    @step\n",
    "    async def new_user_msg(self, ctx: Context, ev: StartEvent) -> PrepEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # clear current reasoning\n",
    "        await ctx.set(\"current_reasoning\", [])\n",
    "\n",
    "        return PrepEvent() #Signals the workflow to proceed to the next step (prepare_chat_history) by emitting a PrepEvent.\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> InputEvent:\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        current_reasoning = await ctx.get(\"current_reasoning\", default=[])\n",
    "        llm_input = self.formatter.format(\n",
    "            self.tools, chat_history, current_reasoning=current_reasoning\n",
    "        )\n",
    "        return InputEvent(input=llm_input)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        response = await self.llm.achat(chat_history)\n",
    "\n",
    "        try:\n",
    "            reasoning_step = self.output_parser.parse(response.message.content)\n",
    "            (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                reasoning_step\n",
    "            )\n",
    "            if reasoning_step.is_done:\n",
    "                # Condition: reasoning_step.is_done checks if the LLM provided a final answer.\n",
    "                # Action:\n",
    "                    # Stores the assistant's response (reasoning_step.response) in memory.\n",
    "                    # Emits a StopEvent with:\n",
    "                        # The final response.\n",
    "                        # Any sources consulted.\n",
    "                        # The reasoning chain from ctx.\n",
    "                self.memory.put(\n",
    "                    ChatMessage(\n",
    "                        role=\"assistant\", content=reasoning_step.response\n",
    "                    )\n",
    "                )\n",
    "                #  RETURN StopEvent if the LLM provides a final response.\n",
    "\n",
    "                return StopEvent(\n",
    "                    result={\n",
    "                        \"response\": reasoning_step.response,\n",
    "                        \"sources\": [*self.sources],\n",
    "                        \"reasoning\": await ctx.get(\n",
    "                            \"current_reasoning\", default=[]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "            elif isinstance(reasoning_step, ActionReasoningStep):\n",
    "                # Condition: The reasoning step indicates the LLM wants to call a tool.\n",
    "                # Action:\n",
    "                    # Extracts the tool_name and tool_args from the reasoning.\n",
    "                    # Emits a ToolCallEvent with:\n",
    "                        # A list of tool calls.\n",
    "                        # Tools are identified by tool_name and passed tool_args.\n",
    "                tool_name = reasoning_step.action\n",
    "                tool_args = reasoning_step.action_input\n",
    "                \n",
    "                #RETURN ToolCallEvent if the LLM decides to invoke a tool.\n",
    "\n",
    "                return ToolCallEvent(\n",
    "                    tool_calls=[\n",
    "                        ToolSelection(\n",
    "                            tool_id=\"fake\",\n",
    "                            tool_name=tool_name,\n",
    "                            tool_kwargs=tool_args,\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "        except Exception as e:\n",
    "            # Appends an ObservationReasoningStep to current_reasoning, noting the error.\n",
    "            # Allows the workflow to continue iterating.\n",
    "            (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                ObservationReasoningStep(\n",
    "                    observation=f\"There was an error in parsing my reasoning: {e}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If parsing fails or neither condition is met, the step returns to a fresh state with a PrepEvent.\n",
    "        # if no tool calls or final response, iterate again\n",
    "        # Condition: If the LLM doesn’t provide a final response or tool call.\n",
    "        # Action: Returns a PrepEvent to loop back and refine the prompt.\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> PrepEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            if not tool:\n",
    "                (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                    ObservationReasoningStep(\n",
    "                        observation=f\"Tool {tool_call.tool_name} does not exist\"\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                    ObservationReasoningStep(observation=tool_output.content)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                    ObservationReasoningStep(\n",
    "                        observation=f\"Error calling tool {tool.metadata.get_name()}: {e}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        \n",
    "        # prep the next iteration\n",
    "        # After processing all tool calls, emits a PrepEvent to continue the reasoning loop.\n",
    "        # This ensures the agent is ready for the next reasoning iteration.\n",
    "\n",
    "        return PrepEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it! Let's explore the workflow we wrote a bit.\n",
    "\n",
    "**new_user_msg():** Adds the user message to memory, and clears the global context to keep track of a fresh string of reasoning.\n",
    "\n",
    "**prepare_chat_history():** Prepares the react prompt, using the chat history, tools, and current reasoning (if any)\n",
    "\n",
    "**handle_llm_input():** Prompts the LLM with our react prompt, and uses some utility functions to parse the output. If there are no tool calls, we can stop and emit a StopEvent. Otherwise, we emit a ToolCallEvent to handle tool calls. Lastly, if there are no tool calls, and no final response, we simply loop again.\n",
    "\n",
    "**handle_tool_calls():** Safely calls tools with error handling, adding the tool outputs to the current reasoning. Then, by emitting a PrepEvent, we loop around for another round of ReAct prompting and parsing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: With loops, we need to be mindful of runtime. Here, we set a timeout of 120s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step new_user_msg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step handle_llm_input produced event StopEvent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Useful function to add two numbers.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Useful function to multiply two numbers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(add),\n",
    "    FunctionTool.from_defaults(multiply),\n",
    "]\n",
    "\n",
    "agent = ReActAgent(\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\"), tools=tools, timeout=120, verbose=True\n",
    ")\n",
    "\n",
    "ret = await agent.run(input=\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(ret[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step new_user_msg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step handle_llm_input produced event StopEvent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n",
      "Failed to export batch code: 401, reason: \n"
     ]
    }
   ],
   "source": [
    "ret = await agent.run(input=\"Calculate: (120*4) + 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of (120 * 4) + 15 is 495.\n"
     ]
    }
   ],
   "source": [
    "print(ret[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
